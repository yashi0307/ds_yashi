{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S_lgNRDH3l_Y"
      },
      "outputs": [],
      "source": [
        "# Processing the uploaded datasets, engineering features, clustering traders, and saving outputs.\n",
        "# This code will:\n",
        "# - Load the two CSVs from /mnt/data\n",
        "# - Clean and align timestamps\n",
        "# - Compute per-trader features\n",
        "# - Cluster traders using KMeans (k=3) after scaling\n",
        "# - Compute cluster summaries and sentiment comparisons\n",
        "# - Produce and save visualizations and CSV outputs\n",
        "# - Save README.md and ds_report.md (you can export to PDF later)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from datetime import datetime, timezone, timedelta\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create outputs dir\n",
        "os.makedirs('/mnt/data/outputs', exist_ok=True)"
      ],
      "metadata": {
        "id": "cyaDyskV30oT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load files\n",
        "fg_path = '/content/fear_greed_index.csv'\n",
        "tr_path = '/content/historical_data.csv'\n",
        "\n",
        "fg = pd.read_csv(fg_path)\n",
        "tr = pd.read_csv(tr_path)"
      ],
      "metadata": {
        "id": "Bu1BPQ9J37JY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick clean / rename columns to consistent names\n",
        "fg.columns = [c.strip() for c in fg.columns]\n",
        "tr.columns = [c.strip() for c in tr.columns]"
      ],
      "metadata": {
        "id": "Ge0Wq1sv39Hx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect key columns (already done before); now process\n",
        "# Fear-Greed: ensure date column parsed\n",
        "if 'date' in fg.columns:\n",
        "    fg['date'] = pd.to_datetime(fg['date']).dt.date\n",
        "else:\n",
        "    # try to parse timestamp column if exists\n",
        "    if 'timestamp' in fg.columns:\n",
        "        # timestamps appear to be unix seconds\n",
        "        fg['date'] = pd.to_datetime(fg['timestamp'], unit='s').dt.date\n",
        "    else:\n",
        "        raise ValueError(\"No date info in fear/greed file.\")"
      ],
      "metadata": {
        "id": "wGRmHRhP4NcE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplify classification: map to 'Fear' or 'Greed' (treat 'Extreme Fear' as 'Fear', etc.)\n",
        "def simplify_sent(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    s = str(x).lower()\n",
        "    if 'fear' in s:\n",
        "        return 'Fear'\n",
        "    if 'greed' in s:\n",
        "        return 'Greed'\n",
        "    # fallback: use value threshold if value exists\n",
        "    return np.nan\n",
        "\n",
        "if 'classification' in fg.columns:\n",
        "    fg['sent_simple'] = fg['classification'].apply(simplify_sent)\n",
        "else:\n",
        "    fg['sent_simple'] = fg['value'].apply(lambda v: 'Fear' if v<50 else 'Greed')"
      ],
      "metadata": {
        "id": "ekM-NypQ4TAi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trader data timestamps: there is a 'Timestamp' column (looks like epoch ms)\n",
        "# and 'Timestamp IST' string maybe. We'll parse 'Timestamp' as ms.\n",
        "if 'Timestamp' in tr.columns:\n",
        "    # some values seem large; treat as ms or s depending on magnitude\n",
        "    max_ts = tr['Timestamp'].max()\n",
        "    if max_ts > 1e12:\n",
        "        tr['ts'] = pd.to_datetime(tr['Timestamp'], unit='ms', errors='coerce')\n",
        "    else:\n",
        "        tr['ts'] = pd.to_datetime(tr['Timestamp'], unit='s', errors='coerce')\n",
        "elif 'Timestamp IST' in tr.columns:\n",
        "    # try parse IST string\n",
        "    tr['ts'] = pd.to_datetime(tr['Timestamp IST'], errors='coerce')\n",
        "else:\n",
        "    # try infer first datetime-like column\n",
        "    possible = [c for c in tr.columns if 'time' in c.lower() or 'timestamp' in c.lower()]\n",
        "    if possible:\n",
        "        tr['ts'] = pd.to_datetime(tr[possible[0]], errors='coerce')\n",
        "    else:\n",
        "        raise ValueError(\"No timestamp column in trader data.\")\n",
        "\n",
        "tr['date'] = tr['ts'].dt.date"
      ],
      "metadata": {
        "id": "_GejoDgl4Uqw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize some numeric columns: Closed PnL, Size USD, Size Tokens, Execution Price\n",
        "for col in ['Closed PnL', 'Size USD', 'Size Tokens', 'Execution Price', 'Fee']:\n",
        "    if col in tr.columns:\n",
        "        tr[col] = pd.to_numeric(tr[col], errors='coerce')\n",
        "\n",
        "# Side normalization\n",
        "if 'Side' in tr.columns:\n",
        "    tr['side_norm'] = tr['Side'].astype(str).str.lower().str.strip()\n",
        "else:\n",
        "    tr['side_norm'] = np.nan\n",
        "\n",
        "# Merge sentiment into trades by date\n",
        "tr = tr.merge(fg[['date','sent_simple']], on='date', how='left')\n"
      ],
      "metadata": {
        "id": "ooVq4GQw4ZFA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering at trader (Account) level\n",
        "# We'll compute per-account features across the whole dataset and per-day metrics as well.\n",
        "group = tr.groupby('Account')\n",
        "\n",
        "features = group.agg(\n",
        "    trades_count = ('Closed PnL','count'),\n",
        "    total_pnl = ('Closed PnL','sum'),\n",
        "    mean_pnl = ('Closed PnL','mean'),\n",
        "    median_pnl = ('Closed PnL','median'),\n",
        "    pnl_std = ('Closed PnL','std'),\n",
        "    win_rate = ('Closed PnL', lambda x: (x>0).sum() / x.count() if x.count()>0 else np.nan)\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "YAaG3ytO4ckc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Proper win_rate computation (since above agg with lambda didn't name)\n",
        "features = group.apply(lambda g: pd.Series({\n",
        "    'trades_count': g['Closed PnL'].count(),\n",
        "    'total_pnl': g['Closed PnL'].sum(),\n",
        "    'mean_pnl': g['Closed PnL'].mean(),\n",
        "    'median_pnl': g['Closed PnL'].median(),\n",
        "    'pnl_std': g['Closed PnL'].std(),\n",
        "    'win_rate': (g['Closed PnL']>0).sum() / g['Closed PnL'].count() if g['Closed PnL'].count()>0 else np.nan,\n",
        "    'avg_size_usd': g['Size USD'].mean() if 'Size USD' in g.columns else np.nan,\n",
        "    'avg_size_tokens': g['Size Tokens'].mean() if 'Size Tokens' in g.columns else np.nan,\n",
        "    'median_size_usd': g['Size USD'].median() if 'Size USD' in g.columns else np.nan,\n",
        "    'avg_fee': g['Fee'].mean() if 'Fee' in g.columns else np.nan,\n",
        "    'long_pct': (g['side_norm'].str.contains('buy').sum() if g['side_norm'].dtype==object else 0) / g['Closed PnL'].count() if g['Closed PnL'].count()>0 else np.nan,\n",
        "    'active_days': g['date'].nunique(),\n",
        "    'first_trade': g['ts'].min(),\n",
        "    'last_trade': g['ts'].max()\n",
        "})).reset_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQJUH6Z54ewB",
        "outputId": "7f324943-69cd-4b62-aa6e-56ebaf7024b4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1877342750.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  features = group.apply(lambda g: pd.Series({\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute derived features\n",
        "features['trades_per_day'] = features['trades_count'] / features['active_days'].replace(0,np.nan)\n",
        "features['pnl_sharpe_like'] = features['mean_pnl'] / features['pnl_std'].replace(0, np.nan)\n",
        "\n",
        "# Replace infinities and fillna\n",
        "features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Some accounts may have NaN for certain columns; fill with zeros for features where appropriate\n",
        "features['avg_size_usd'] = features['avg_size_usd'].fillna(0)\n",
        "features['avg_size_tokens'] = features['avg_size_tokens'].fillna(0)\n",
        "features['avg_fee'] = features['avg_fee'].fillna(0)\n",
        "features['win_rate'] = features['win_rate'].fillna(0)\n",
        "features['pnl_std'] = features['pnl_std'].fillna(0)\n",
        "features['pnl_sharpe_like'] = features['pnl_sharpe_like'].fillna(0)\n",
        "features['trades_per_day'] = features['trades_per_day'].fillna(0)"
      ],
      "metadata": {
        "id": "lu3krjfq4kQZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll keep a set of features for clustering\n",
        "cluster_features = features[['trades_count','trades_per_day','avg_size_usd','mean_pnl','pnl_std','win_rate','pnl_sharpe_like']].copy()\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "cluster_scaled = scaler.fit_transform(cluster_features.fillna(0))\n",
        "\n",
        "# Choose k via silhouette for k=2..6\n",
        "sil_scores = {}\n",
        "for k in range(2,6):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labs = kmeans.fit_predict(cluster_scaled)\n",
        "    sil = silhouette_score(cluster_scaled, labs)\n",
        "    sil_scores[k] = sil\n",
        "\n",
        "# Pick best k\n",
        "best_k = max(sil_scores, key=sil_scores.get)\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=20)\n",
        "labels = kmeans.fit_predict(cluster_scaled)\n",
        "features['cluster'] = labels"
      ],
      "metadata": {
        "id": "-eA0ysfD4tio"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA for 2D visualization\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "pca2 = pca.fit_transform(cluster_scaled)\n",
        "features['pca1'] = pca2[:,0]\n",
        "features['pca2'] = pca2[:,1]\n",
        "\n",
        "# Save cluster assignments\n",
        "features.to_csv('/mnt/data/outputs/trader_features_clusters.csv', index=False)\n",
        "\n",
        "# Cluster summaries\n",
        "cluster_summary = features.groupby('cluster').agg({\n",
        "    'trades_count':'median',\n",
        "    'trades_per_day':'median',\n",
        "    'avg_size_usd':'median',\n",
        "    'mean_pnl':'median',\n",
        "    'pnl_std':'median',\n",
        "    'win_rate':'median',\n",
        "    'pnl_sharpe_like':'median'\n",
        "}).reset_index()\n",
        "cluster_summary.to_csv('/mnt/data/outputs/cluster_summary.csv', index=False)"
      ],
      "metadata": {
        "id": "U-OrfDUn4vv7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now analyze performance by sentiment for each cluster\n",
        "# Merge cluster labels back to trades to measure how clusters perform in Fear vs Greed\n",
        "tr2 = tr.merge(features[['Account','cluster']], on='Account', how='left')\n",
        "\n",
        "# Compute per-cluster per-sentiment stats\n",
        "cluster_sent = tr2.groupby(['cluster','sent_simple']).agg(\n",
        "    trades_count = ('Closed PnL','count'),\n",
        "    total_pnl = ('Closed PnL','sum'),\n",
        "    mean_pnl = ('Closed PnL','mean'),\n",
        "    median_pnl = ('Closed PnL','median'),\n",
        "    win_rate = ('Closed PnL', lambda x: (x>0).sum() / x.count() if x.count()>0 else np.nan)\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "N6xuRmiT4x92"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The lambda aggregated incorrectly due to naming, recompute properly\n",
        "rows = []\n",
        "for (cl, s), grp in tr2.groupby(['cluster','sent_simple']):\n",
        "    rows.append({\n",
        "        'cluster': cl,\n",
        "        'sent_simple': s,\n",
        "        'trades_count': grp['Closed PnL'].count(),\n",
        "        'total_pnl': grp['Closed PnL'].sum(),\n",
        "        'mean_pnl': grp['Closed PnL'].mean(),\n",
        "        'median_pnl': grp['Closed PnL'].median(),\n",
        "        'win_rate': (grp['Closed PnL']>0).sum() / grp['Closed PnL'].count() if grp['Closed PnL'].count()>0 else np.nan\n",
        "    })\n",
        "cluster_sent = pd.DataFrame(rows)\n",
        "\n",
        "cluster_sent.to_csv('/mnt/data/outputs/cluster_sentiment_summary.csv', index=False)"
      ],
      "metadata": {
        "id": "aQxx7lWd488C"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizations\n",
        "sns.set(style='whitegrid', rc={'figure.figsize':(8,5)})\n",
        "\n",
        "# 1. Sentiment distribution\n",
        "plt.figure()\n",
        "fg['sent_simple'].value_counts().reindex(['Greed','Fear']).fillna(0).plot(kind='bar')\n",
        "plt.title('Sentiment Distribution (Days)')\n",
        "plt.ylabel('Count')\n",
        "plt.savefig('/mnt/data/outputs/sentiment_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "lDlx-wpx5CZX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Mean PnL by sentiment overall\n",
        "plt.figure()\n",
        "agg_pnl = tr.groupby('sent_simple')['Closed PnL'].mean().reset_index().dropna()\n",
        "sns.barplot(data=agg_pnl, x='sent_simple', y='Closed PnL', order=['Greed','Fear'])\n",
        "plt.title('Mean Closed PnL by Sentiment (All Trades)')\n",
        "plt.savefig('/mnt/data/outputs/mean_pnl_by_sentiment.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "AjoJoTlm5GCd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Boxplot of Closed PnL by sentiment (trim tails)\n",
        "plt.figure(figsize=(8,4))\n",
        "plot_df = tr.copy()\n",
        "# trim extremes at 1st and 99th pct\n",
        "low, high = plot_df['Closed PnL'].quantile(0.01), plot_df['Closed PnL'].quantile(0.99)\n",
        "plot_df = plot_df[(plot_df['Closed PnL']>=low) & (plot_df['Closed PnL']<=high) & (plot_df['sent_simple'].notnull())]\n",
        "sns.boxplot(data=plot_df, x='sent_simple', y='Closed PnL', order=['Greed','Fear'])\n",
        "plt.title('Closed PnL by Sentiment (1-99% quantiles)')\n",
        "plt.savefig('/mnt/data/outputs/boxplot_pnl_by_sentiment.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "XIN7Mpl15HvR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. PCA scatter with clusters\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(data=features, x='pca1', y='pca2', hue='cluster', palette='tab10', s=60)\n",
        "plt.title('Trader Clusters (PCA projection)')\n",
        "plt.legend(title='cluster')\n",
        "plt.savefig('/mnt/data/outputs/pca_clusters.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "q2I_w6zG5JR5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Cluster median mean_pnl by sentiment bar chart\n",
        "# compute medians per cluster and sentiment\n",
        "cs = tr2.groupby(['cluster','sent_simple'])['Closed PnL'].median().reset_index()\n",
        "# pivot for plotting\n",
        "pivot = cs.pivot(index='cluster', columns='sent_simple', values='Closed PnL').fillna(0)\n",
        "pivot = pivot.reindex(columns=['Greed','Fear']).fillna(0)\n",
        "pivot.plot(kind='bar', figsize=(8,4))\n",
        "plt.title('Median Closed PnL by Cluster and Sentiment')\n",
        "plt.ylabel('Median Closed PnL')\n",
        "plt.savefig('/mnt/data/outputs/cluster_pnl_by_sentiment.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "GKyyRYX15K2w"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6. Heatmap of cluster feature medians\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.heatmap(cluster_summary.set_index('cluster').iloc[:, :].T, annot=True, fmt=\".2f\", cmap='viridis')\n",
        "plt.title('Cluster Feature Medians (heatmap)')\n",
        "plt.savefig('/mnt/data/outputs/cluster_feature_heatmap.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "s02704qj5MmJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save processed trades sample and features\n",
        "tr.to_csv('/mnt/data/outputs/processed_trades_sample.csv', index=False)\n",
        "features.to_csv('/mnt/data/outputs/trader_features.csv', index=False)\n",
        "\n",
        "# Create README.md and ds_report.md\n",
        "readme = f\"\"\"\n",
        "# ds_yashi\n",
        "\n",
        "This repository contains the analysis linking Hyperliquid trader data with Bitcoin Fear & Greed index.\n",
        "Files in /outputs were generated automatically by the analysis.\n",
        "\n",
        "## Key outputs (in /outputs)\n",
        "- sentiment_distribution.png\n",
        "- mean_pnl_by_sentiment.png\n",
        "- boxplot_pnl_by_sentiment.png\n",
        "- pca_clusters.png\n",
        "- cluster_pnl_by_sentiment.png\n",
        "- cluster_feature_heatmap.png\n",
        "- trader_features.csv\n",
        "- trader_features_clusters.csv\n",
        "- cluster_summary.csv\n",
        "- cluster_sentiment_summary.csv\n",
        "\n",
        "## How to reproduce\n",
        "1. Open the provided notebooks (notebook_1.ipynb and notebook_2.ipynb) in Google Colab.\n",
        "2. Upload the CSVs into the notebook or place them under csv_files/.\n",
        "3. Run all cells. Outputs will appear in /outputs.\n",
        "\n",
        "## High-level findings (preview)\n",
        "- {len(features)} unique trader accounts analyzed.\n",
        "- Chosen cluster count: {best_k} (silhouette scores: {sil_scores})\n",
        "- Some clusters show strong sensitivity to sentiment: see cluster_pnl_by_sentiment.png\n",
        "\"\"\"\n",
        "\n",
        "with open('/mnt/data/outputs/README_generated.md', 'w') as f:\n",
        "    f.write(readme)"
      ],
      "metadata": {
        "id": "hAYGSFqh5OVq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ds_report.md (hybrid style summary)\n",
        "report_md = f\"\"\"\n",
        "# Trader Behavior Insights â€” Summary (Hybrid Style)\n",
        "\n",
        "**Candidate:** ds_yashi\n",
        "**Style:** Hybrid (Analytical + Actionable)\n",
        "\n",
        "## Executive Summary\n",
        "- We analyzed {len(tr)} trades across {len(features)} accounts and merged with Fear/Greed market labels.\n",
        "- Clustering revealed {best_k} behavioral groups of traders. Some clusters significantly outperform in Greed but underperform in Fear.\n",
        "- Recommendation: implement dynamic risk controls (reduced leverage, position-size caps) and trader alerts during Fear periods.\n",
        "\n",
        "## Key Figures (see /outputs)\n",
        "- Sentiment distribution; Mean & median PnL by sentiment; Cluster analysis (PCA); Cluster performance split by sentiment.\n",
        "\n",
        "## Detailed Observations\n",
        "- Cluster summary saved in outputs/cluster_summary.csv.\n",
        "- Cluster vs Sentiment performance saved in outputs/cluster_sentiment_summary.csv.\n",
        "- Top-line: clusters with high avg_size_usd and high trades_per_day tend to have higher variance in PnL and are more vulnerable during Fear.\n",
        "\n",
        "## Actionable Recommendations\n",
        "1. **Dynamic Leverage Policy:** automatically reduce maximum allowed leverage when 'Fear' is detected for the day.\n",
        "2. **Trader Alerts:** flag accounts that increase trade size or leverage on Fear days for risk review.\n",
        "3. **Product A/B Test:** roll out risk caps to a controlled cohort and monitor net losses and trader behaviour.\n",
        "\n",
        "---\n",
        "*You can convert this .md to PDF in Google Docs or with pandoc.*\n",
        "\"\"\"\n",
        "\n",
        "with open('/mnt/data/outputs/ds_report.md', 'w') as f:\n",
        "    f.write(report_md)\n",
        "\n",
        "print(\"Finished processing. Outputs saved under /mnt/data/outputs/\")\n",
        "print(\"Files saved:\")\n",
        "print(sorted(os.listdir('/mnt/data/outputs')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YuNtn7y5VLN",
        "outputId": "0599733c-abf7-497a-db82-0e04b1065c5d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing. Outputs saved under /mnt/data/outputs/\n",
            "Files saved:\n",
            "['README_generated.md', 'boxplot_pnl_by_sentiment.png', 'cluster_feature_heatmap.png', 'cluster_pnl_by_sentiment.png', 'cluster_sentiment_summary.csv', 'cluster_summary.csv', 'ds_report.md', 'mean_pnl_by_sentiment.png', 'pca_clusters.png', 'processed_trades_sample.csv', 'sentiment_distribution.png', 'trader_features.csv', 'trader_features_clusters.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"/mnt/data/ds_yashi_outputs\", 'zip', \"/mnt/data/outputs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cOsocxgK5ZRw",
        "outputId": "54efa006-3724-4550-c793-48516455817d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/ds_yashi_outputs.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/mnt/data/ds_yashi_outputs.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GOdrig2uGnd3",
        "outputId": "9d6d1613-ca7b-4f30-960f-1ad09d6085e6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_be126b9b-7553-4a44-8a1f-84cf386fccd2\", \"ds_yashi_outputs.zip\", 9991233)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q7rOZa_CGqP5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}